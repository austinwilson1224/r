---
title: Predicting a known function
date: 8 November 2020
---

#### Outcomes

- Fit statistical learning models to univariate data
- Plot fitted models
- Interpret models


#### Instructions

- Answer the following questions, and show all your R code.
- Upload your submission to Canvas in nicely formatted HTML generated from Rstudio.


## generating simulated data

Choose `n` between 30 and 200, and sample `n` values for `x` from a random uniform (0, 1) distribution.
Define `y` corresponding to `x` from the following quadratic function:

$$
y = 5x^2 - 4x - 10 + \epsilon
$$

Here Îµ is normally distributed with mean 0 and standard deviation 0.1.
Plot your data.

```{r}
set.seed(138)
x = runif(n = 300, min = 0, max = 1)
epsilon = rnorm(n = 300, mean = 0, sd = .1)
y = 5 * (x ** 2) - 4 * x + epsilon
d = data.frame(x,y)
plot(x,y)
```

## linear model

Use `fit1 = lm(y ~ x)` to fit a linear model to the data.
What mathematical function of `x` does the fitted model represent?
Implement the fitted model as a function in R, and verify that it matches the values predicted by the model.

it looks like a quadratic function

$$
y = ax^2 + b
$$

Hint: you can do something like the following:
```{r}
fit1 = lm(y ~ x)
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)
y_pred = predict(fit1, data.frame(x = x1))
lines(x1,y_pred, col = "red")
```


## quadratic model

Create a new linear model than includes a quadratic `x^2` term, for example, using `lm(y ~ x + I(x^2))`.
What mathematical function of `x` does the fitted model represent?

```{r}
fit1 = lm(y ~ x + I(x^2))
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)
y_pred = predict(fit1, data.frame(x = x1))
lines(x1,y_pred, col = "red")
```



## comparing models

Plot lines for the linear and quadratic model together with the data points.
Which appears to do a better job fitting the data?
Explain.

```{r}
fit1 = lm(y ~ x)
fit2 = lm(y ~ x + I(x^2))
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)

y_pred1 = predict(fit1, data.frame(x = x1))
y_pred2 = predict(fit2, data.frame(x = x1))

lines(x1,y_pred1, col = "red")
lines(x1,y_pred2, col = "red")
```

## recursive partitioning

- Fit a recursive partitioning model to the data using `rpart`.
- Experiment with the parameters of the algorithm by passing different parameters to the algorithm, see `?rpart.control`.
- Plot and compare two different models from `rpart` for this data set.
- Which parameters appear to make the recursive partitioning model fit better or worse on this data set?

```{r}
set.seed(138)
x = runif(n = 300, min = 0, max = 1)
epsilon = rnorm(n = 300, mean = 0, sd = .1)
y = 5 * (x ** 2) - 4 * x + epsilon



fit3 = rpart(y ~ x + I(x^2),  data = d, minsplit = 20)
fit4 = rpart(y ~ x, minsplit=20,minbucket=20)
fit5 = rpart(y ~ x, maxdepth = 20)
# yhat = predict(fit)
# test = data.frame(x,yhat)
# fit = smooth.spline(test)
# plot(x,yhat)

#lines(order(x),yhat, type = "l")
#rpart.plot(fit4, type = 0)

```



## test data performance

Simulate more values from the true model

$$
y = 5x^2 - 4x - 10 + \epsilon
$$
where `x` is between 0 and 1.

```{r}
x_test = runif(n = 1000, min = 0, max = 1)


f_x = function(x, n = 100, min = 0, max = 1) {
  # x = runif(n = n, min = min, max = max)
  epsilon = rnorm(n = n, mean = 0, sd = .1)
  5 * (x ** 2) - 4 * x + epsilon
}

y_test = f_x(x_test,n = 1000)
d_test = data.frame(x_test,y_test)
# y
# plot(x_test,y_true)
```


Compare the performance of three different models (linear, quadratic, and recursive partitioning) on this test set.
Which model does the best job minimizing the sum of squared error?

```{r}
mse = function(model, test_data) {
  predictions = predict(model,test_data)
  error = predictions - test_data$y
  squared = error ** 2
  mean = mean(squared)
  mean
}

mse_linear = mse(fit1,d_test)
mse_quadratic = mse(fit2,d_test)
mse_rpart1 = mse(fit3,d_test)
mse_rpart2 = mse(fit4,d_test)
mse_rpart3 = mse(fit5,d_test)
mse_linear
mse_quadratic
mse_rpart1
mse_rpart2 # best performance 
mse_rpart3
```



## a data set to suit the model

Simulate a slightly noisy data set where the recursive partitioning model should perform much better than the simple linear model.
What characteristics of the data make the recursive partitioning model work well?
Fit and plot both a linear model and a recursive partitioning model on the same plot for this data to demonstrate that recursive partitioning performs better.



```{r}
x = runif(n = 1000, min = 0, max = 1)
noise = rnorm(n = 1000, mean = 0, sd = .5)
y = 3 * x + 5 + noise
d = data.frame(x,y)
d = d[order(x),]
fit_linear = lm(y ~ x, data = d)
fit_rpart = rpart(y ~ x, data  = d, minsplit=20,minbucket=20)
plot(x,y)
# sort(d$x)
lines(d$x,predict(fit_linear,d),col = "red")
lines(sort(d$x),predict(fit_rpart,d), col = "red")
```

