---
title: Predicting a known function
date: 8 November 2020
---

#### Outcomes

- Fit statistical learning models to univariate data
- Plot fitted models
- Interpret models


#### Instructions

- Answer the following questions, and show all your R code.
- Upload your submission to Canvas in nicely formatted HTML generated from Rstudio.


## generating simulated data

Choose `n` between 30 and 200, and sample `n` values for `x` from a random uniform (0, 1) distribution.
Define `y` corresponding to `x` from the following quadratic function:

$$
y = 5x^2 - 4x - 10 + \epsilon
$$

Here Îµ is normally distributed with mean 0 and standard deviation 0.1.
Plot your data.

```{r}
set.seed(138)
x = runif(n = 300, min = 0, max = 1)
epsilon = rnorm(n = 300, mean = 0, sd = .1)
y = 5 * (x ** 2) - 4 * x + epsilon
d = data.frame(x,y)
plot(x,y)
```

## linear model

Use `fit1 = lm(y ~ x)` to fit a linear model to the data.
What mathematical function of `x` does the fitted model represent?
Implement the fitted model as a function in R, and verify that it matches the values predicted by the model.

it looks like a quadratic function

$$
y = ax^2 + b
$$

Hint: you can do something like the following:
```{r}
linear_fit = lm(y ~ x)
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)
y_pred = predict(linear_fit, data.frame(x = x1))
lines(x1,y_pred, col = "red")
```


## quadratic model

Create a new linear model than includes a quadratic `x^2` term, for example, using `lm(y ~ x + I(x^2))`.
What mathematical function of `x` does the fitted model represent?

```{r}
quadratic_fit = lm(y ~ x + I(x^2))
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)
y_pred = predict(quadratic_fit, data.frame(x = x1))
lines(x1,y_pred, col = "red")
```

- This looks like a quadratic function, which is what we created in the previous problem. 

## comparing models

Plot lines for the linear and quadratic model together with the data points.
Which appears to do a better job fitting the data?
Explain.

```{r}
# fit1 = lm(y ~ x)
# fit2 = lm(y ~ x + I(x^2))
x1 = seq(from = 0, to = 29, by = 0.1)
plot(x,y)

y_pred1 = predict(linear_fit, data.frame(x = x1))
y_pred2 = predict(quadratic_fit, data.frame(x = x1))

lines(x1,y_pred1, col = "red")
lines(x1,y_pred2, col = "red")
```

- The quatratic model is better at fitting the data. This is because the line is closer the the trend of the data. The difference between our prediction and the actual value is smaller than with the linear model. The linear model is very inaccurate except in two places. (.2 and .8)

## recursive partitioning

- Fit a recursive partitioning model to the data using `rpart`.
- Experiment with the parameters of the algorithm by passing different parameters to the algorithm, see `?rpart.control`.
- Plot and compare two different models from `rpart` for this data set.
- Which parameters appear to make the recursive partitioning model fit better or worse on this data set?

```{r}
library(rpart)
x = sort(runif(n = 1000, min = 0, max = 1))
epsilon = rnorm(n = 1000, mean = 0, sd = .1)
y = 5 * (x ** 2) - 4 * x + epsilon


d = data.frame(x,y)
d = d[order(x),]


fit = rpart(y ~ x + I(x^2), data = d)
fit1 = rpart(y ~ x + I(x^2),  data = d, minsplit = 20)
fit2 = rpart(y ~ x + I(x^2), data = d, minsplit=20,minbucket=200,maxdepth=20)
fit3 = rpart(y ~ x + I(x^2),  data = d, minbucket = 5)



with(d,plot(x,y))
lines(d$x,predict(fit), col = "red")
lines(d$x,predict(fit1), col = "blue")
lines(d$x,predict(fit2), col = "green")
lines(d$x,predict(fit3))




```


All of my lines are very similar. Changing minbucket is the only parameter that makes a significant change to the model. This actually causes the model to degrade in performance. The black line is the same line for 3 different model parameters. (fit, fit1,fit3) The default parameters work great for this model. Increasing min bucket makes the model worse.

## test data performance

Simulate more values from the true model

$$
y = 5x^2 - 4x - 10 + \epsilon
$$
where `x` is between 0 and 1.

```{r}
x_test = sort(runif(n = 1000, min = 0, max = 1))


f_x = function(x, n = 100, min = 0, max = 1) {
  epsilon = rnorm(n = n, mean = 0, sd = .1)
  5 * (x ** 2) - 4 * x + epsilon
}

y_test = f_x(x_test,n = 1000)
d = data.frame(x_test,y_test)
```


Compare the performance of three different models (linear, quadratic, and recursive partitioning) on this test set.
Which model does the best job minimizing the sum of squared error?

```{r}
mse = function(model, d) {
  mean((predict(model,d) - d$y) ** 2)
}

mse_linear = mse(linear_fit,d)
mse_quadratic = mse(quadratic_fit,d)
mse_rpart = mse(fit,d)
mse_rpart1 = mse(fit1,d)
mse_rpart2 = mse(fit2,d)
mse_linear
mse_quadratic # best performance 
mse_rpart1
mse_rpart2 
```

- the quadratic moedal has the best performance. MSE for quadtric is an order of magnitude smaller than the others. 

## a data set to suit the model

Simulate a slightly noisy data set where the recursive partitioning model should perform much better than the simple linear model.
What characteristics of the data make the recursive partitioning model work well?
Fit and plot both a linear model and a recursive partitioning model on the same plot for this data to demonstrate that recursive partitioning performs better.



```{r}
x = runif(n = 100, min = -10, max = 10)
noise = rnorm(n = 100, mean = 0, sd = 10)
# y = 3 * x + 5 + noise
# y =  4 * x ** 4 + (3-x) ** 3 + x ** 2 + 3 + noise + sin(x)
y = x ** 3 + noise
d = data.frame(x,y)
d = d[order(x),]
linear_fit = lm(y ~ x, data = d)
rpart_fit = rpart(y ~ x, data  = d)
cubic_fit = lm(y ~ x + I(x^2) + I(x^3), data = d)
plot(x,y)
# sort(d$x)
lines(d$x,predict(linear_fit,d),col = "red")
lines(d$x,predict(rpart_fit,d), col = "red")
lines(d$x,predict(cubic_fit,d), col = "red")
mse_linear = mse(linear_fit,d)
mse_rpart = mse(rpart_fit,d)
mse_cubic = mse(cubic_fit,d)
mse_linear 
mse_rpart # lower
mse_cubic # lowest 
```

- rpart has lower MSE than a simple linear model. The cubic model has the lowest because it is a cubic function.The rpart model works better with the data because it is more flexible. The function can go up or down. A simple linear model is very constrained. 
